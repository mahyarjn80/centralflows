{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c390365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch import ViT, SimpleViT\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a4f7b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL PARAMETERS BY DIMENSION\n",
      "================================================================================\n",
      "\n",
      "pos_embedding\n",
      "  Shape: (1, 65, 128), Dimensions: 3, Parameters: 8,320\n",
      "\n",
      "cls_token\n",
      "  Shape: (1, 1, 128), Dimensions: 3, Parameters: 128\n",
      "\n",
      "to_patch_embedding.1.weight\n",
      "  Shape: (48,), Dimensions: 1, Parameters: 48\n",
      "\n",
      "to_patch_embedding.1.bias\n",
      "  Shape: (48,), Dimensions: 1, Parameters: 48\n",
      "\n",
      "to_patch_embedding.2.weight\n",
      "  Shape: (128, 48), Dimensions: 2, Parameters: 6,144\n",
      "\n",
      "to_patch_embedding.2.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "to_patch_embedding.3.weight\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "to_patch_embedding.3.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.norm.weight\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.norm.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.0.0.norm.weight\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.0.0.norm.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.0.0.to_qkv.weight\n",
      "  Shape: (1536, 128), Dimensions: 2, Parameters: 196,608\n",
      "\n",
      "transformer.layers.0.0.to_out.0.weight\n",
      "  Shape: (128, 512), Dimensions: 2, Parameters: 65,536\n",
      "\n",
      "transformer.layers.0.0.to_out.0.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.0.1.net.0.weight\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.0.1.net.0.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.0.1.net.1.weight\n",
      "  Shape: (256, 128), Dimensions: 2, Parameters: 32,768\n",
      "\n",
      "transformer.layers.0.1.net.1.bias\n",
      "  Shape: (256,), Dimensions: 1, Parameters: 256\n",
      "\n",
      "transformer.layers.0.1.net.4.weight\n",
      "  Shape: (128, 256), Dimensions: 2, Parameters: 32,768\n",
      "\n",
      "transformer.layers.0.1.net.4.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.1.0.norm.weight\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.1.0.norm.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.1.0.to_qkv.weight\n",
      "  Shape: (1536, 128), Dimensions: 2, Parameters: 196,608\n",
      "\n",
      "transformer.layers.1.0.to_out.0.weight\n",
      "  Shape: (128, 512), Dimensions: 2, Parameters: 65,536\n",
      "\n",
      "transformer.layers.1.0.to_out.0.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.1.1.net.0.weight\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.1.1.net.0.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.1.1.net.1.weight\n",
      "  Shape: (256, 128), Dimensions: 2, Parameters: 32,768\n",
      "\n",
      "transformer.layers.1.1.net.1.bias\n",
      "  Shape: (256,), Dimensions: 1, Parameters: 256\n",
      "\n",
      "transformer.layers.1.1.net.4.weight\n",
      "  Shape: (128, 256), Dimensions: 2, Parameters: 32,768\n",
      "\n",
      "transformer.layers.1.1.net.4.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.2.0.norm.weight\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.2.0.norm.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.2.0.to_qkv.weight\n",
      "  Shape: (1536, 128), Dimensions: 2, Parameters: 196,608\n",
      "\n",
      "transformer.layers.2.0.to_out.0.weight\n",
      "  Shape: (128, 512), Dimensions: 2, Parameters: 65,536\n",
      "\n",
      "transformer.layers.2.0.to_out.0.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.2.1.net.0.weight\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.2.1.net.0.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.2.1.net.1.weight\n",
      "  Shape: (256, 128), Dimensions: 2, Parameters: 32,768\n",
      "\n",
      "transformer.layers.2.1.net.1.bias\n",
      "  Shape: (256,), Dimensions: 1, Parameters: 256\n",
      "\n",
      "transformer.layers.2.1.net.4.weight\n",
      "  Shape: (128, 256), Dimensions: 2, Parameters: 32,768\n",
      "\n",
      "transformer.layers.2.1.net.4.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.3.0.norm.weight\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.3.0.norm.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.3.0.to_qkv.weight\n",
      "  Shape: (1536, 128), Dimensions: 2, Parameters: 196,608\n",
      "\n",
      "transformer.layers.3.0.to_out.0.weight\n",
      "  Shape: (128, 512), Dimensions: 2, Parameters: 65,536\n",
      "\n",
      "transformer.layers.3.0.to_out.0.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.3.1.net.0.weight\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.3.1.net.0.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.3.1.net.1.weight\n",
      "  Shape: (256, 128), Dimensions: 2, Parameters: 32,768\n",
      "\n",
      "transformer.layers.3.1.net.1.bias\n",
      "  Shape: (256,), Dimensions: 1, Parameters: 256\n",
      "\n",
      "transformer.layers.3.1.net.4.weight\n",
      "  Shape: (128, 256), Dimensions: 2, Parameters: 32,768\n",
      "\n",
      "transformer.layers.3.1.net.4.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.4.0.norm.weight\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.4.0.norm.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.4.0.to_qkv.weight\n",
      "  Shape: (1536, 128), Dimensions: 2, Parameters: 196,608\n",
      "\n",
      "transformer.layers.4.0.to_out.0.weight\n",
      "  Shape: (128, 512), Dimensions: 2, Parameters: 65,536\n",
      "\n",
      "transformer.layers.4.0.to_out.0.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.4.1.net.0.weight\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.4.1.net.0.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.4.1.net.1.weight\n",
      "  Shape: (256, 128), Dimensions: 2, Parameters: 32,768\n",
      "\n",
      "transformer.layers.4.1.net.1.bias\n",
      "  Shape: (256,), Dimensions: 1, Parameters: 256\n",
      "\n",
      "transformer.layers.4.1.net.4.weight\n",
      "  Shape: (128, 256), Dimensions: 2, Parameters: 32,768\n",
      "\n",
      "transformer.layers.4.1.net.4.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.5.0.norm.weight\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.5.0.norm.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.5.0.to_qkv.weight\n",
      "  Shape: (1536, 128), Dimensions: 2, Parameters: 196,608\n",
      "\n",
      "transformer.layers.5.0.to_out.0.weight\n",
      "  Shape: (128, 512), Dimensions: 2, Parameters: 65,536\n",
      "\n",
      "transformer.layers.5.0.to_out.0.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.5.1.net.0.weight\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.5.1.net.0.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "transformer.layers.5.1.net.1.weight\n",
      "  Shape: (256, 128), Dimensions: 2, Parameters: 32,768\n",
      "\n",
      "transformer.layers.5.1.net.1.bias\n",
      "  Shape: (256,), Dimensions: 1, Parameters: 256\n",
      "\n",
      "transformer.layers.5.1.net.4.weight\n",
      "  Shape: (128, 256), Dimensions: 2, Parameters: 32,768\n",
      "\n",
      "transformer.layers.5.1.net.4.bias\n",
      "  Shape: (128,), Dimensions: 1, Parameters: 128\n",
      "\n",
      "mlp_head.weight\n",
      "  Shape: (10, 128), Dimensions: 2, Parameters: 1,280\n",
      "\n",
      "mlp_head.bias\n",
      "  Shape: (10,), Dimensions: 1, Parameters: 10\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "1D Parameters (e.g., biases, layer norms):\n",
      "  Count: 50\n",
      "  Total parameters: 6,890\n",
      "  Names:\n",
      "    - to_patch_embedding.1.weight\n",
      "    - to_patch_embedding.1.bias\n",
      "    - to_patch_embedding.2.bias\n",
      "    - to_patch_embedding.3.weight\n",
      "    - to_patch_embedding.3.bias\n",
      "    - transformer.norm.weight\n",
      "    - transformer.norm.bias\n",
      "    - transformer.layers.0.0.norm.weight\n",
      "    - transformer.layers.0.0.norm.bias\n",
      "    - transformer.layers.0.0.to_out.0.bias\n",
      "    - transformer.layers.0.1.net.0.weight\n",
      "    - transformer.layers.0.1.net.0.bias\n",
      "    - transformer.layers.0.1.net.1.bias\n",
      "    - transformer.layers.0.1.net.4.bias\n",
      "    - transformer.layers.1.0.norm.weight\n",
      "    - transformer.layers.1.0.norm.bias\n",
      "    - transformer.layers.1.0.to_out.0.bias\n",
      "    - transformer.layers.1.1.net.0.weight\n",
      "    - transformer.layers.1.1.net.0.bias\n",
      "    - transformer.layers.1.1.net.1.bias\n",
      "    - transformer.layers.1.1.net.4.bias\n",
      "    - transformer.layers.2.0.norm.weight\n",
      "    - transformer.layers.2.0.norm.bias\n",
      "    - transformer.layers.2.0.to_out.0.bias\n",
      "    - transformer.layers.2.1.net.0.weight\n",
      "    - transformer.layers.2.1.net.0.bias\n",
      "    - transformer.layers.2.1.net.1.bias\n",
      "    - transformer.layers.2.1.net.4.bias\n",
      "    - transformer.layers.3.0.norm.weight\n",
      "    - transformer.layers.3.0.norm.bias\n",
      "    - transformer.layers.3.0.to_out.0.bias\n",
      "    - transformer.layers.3.1.net.0.weight\n",
      "    - transformer.layers.3.1.net.0.bias\n",
      "    - transformer.layers.3.1.net.1.bias\n",
      "    - transformer.layers.3.1.net.4.bias\n",
      "    - transformer.layers.4.0.norm.weight\n",
      "    - transformer.layers.4.0.norm.bias\n",
      "    - transformer.layers.4.0.to_out.0.bias\n",
      "    - transformer.layers.4.1.net.0.weight\n",
      "    - transformer.layers.4.1.net.0.bias\n",
      "    - transformer.layers.4.1.net.1.bias\n",
      "    - transformer.layers.4.1.net.4.bias\n",
      "    - transformer.layers.5.0.norm.weight\n",
      "    - transformer.layers.5.0.norm.bias\n",
      "    - transformer.layers.5.0.to_out.0.bias\n",
      "    - transformer.layers.5.1.net.0.weight\n",
      "    - transformer.layers.5.1.net.0.bias\n",
      "    - transformer.layers.5.1.net.1.bias\n",
      "    - transformer.layers.5.1.net.4.bias\n",
      "    - mlp_head.bias\n",
      "\n",
      "2D Parameters (e.g., weight matrices):\n",
      "  Count: 26\n",
      "  Total parameters: 1,973,504\n",
      "  Names:\n",
      "    - to_patch_embedding.2.weight\n",
      "    - transformer.layers.0.0.to_qkv.weight\n",
      "    - transformer.layers.0.0.to_out.0.weight\n",
      "    - transformer.layers.0.1.net.1.weight\n",
      "    - transformer.layers.0.1.net.4.weight\n",
      "    - transformer.layers.1.0.to_qkv.weight\n",
      "    - transformer.layers.1.0.to_out.0.weight\n",
      "    - transformer.layers.1.1.net.1.weight\n",
      "    - transformer.layers.1.1.net.4.weight\n",
      "    - transformer.layers.2.0.to_qkv.weight\n",
      "    - transformer.layers.2.0.to_out.0.weight\n",
      "    - transformer.layers.2.1.net.1.weight\n",
      "    - transformer.layers.2.1.net.4.weight\n",
      "    - transformer.layers.3.0.to_qkv.weight\n",
      "    - transformer.layers.3.0.to_out.0.weight\n",
      "    - transformer.layers.3.1.net.1.weight\n",
      "    - transformer.layers.3.1.net.4.weight\n",
      "    - transformer.layers.4.0.to_qkv.weight\n",
      "    - transformer.layers.4.0.to_out.0.weight\n",
      "    - transformer.layers.4.1.net.1.weight\n",
      "    - transformer.layers.4.1.net.4.weight\n",
      "    - transformer.layers.5.0.to_qkv.weight\n",
      "    - transformer.layers.5.0.to_out.0.weight\n",
      "    - transformer.layers.5.1.net.1.weight\n",
      "    - transformer.layers.5.1.net.4.weight\n",
      "    - mlp_head.weight\n",
      "\n",
      "Other Dimensions (3D+):\n",
      "  Count: 2\n",
      "  Total parameters: 8,448\n",
      "  Names:\n",
      "    - pos_embedding\n",
      "    - cls_token\n",
      "\n",
      "Total trainable parameters: 1,988,842\n",
      "  1D: 0.35%\n",
      "  2D: 99.23%\n",
      "  Other: 0.42%\n"
     ]
    }
   ],
   "source": [
    "model = ViT(\n",
    "    image_size=32,\n",
    "    patch_size=4,\n",
    "    num_classes=10,\n",
    "    dim=128,\n",
    "    depth=6,\n",
    "    heads=8,\n",
    "    mlp_dim=256,\n",
    "    pool='cls',\n",
    "    dropout=0.0,\n",
    "    emb_dropout=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "# Print all parameters and group them by dimensionality\n",
    "params_1d = []\n",
    "params_2d = []\n",
    "params_other = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL PARAMETERS BY DIMENSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        ndim = param.ndim\n",
    "        shape = tuple(param.shape)\n",
    "        num_params = param.numel()\n",
    "        \n",
    "        print(f\"\\n{name}\")\n",
    "        print(f\"  Shape: {shape}, Dimensions: {ndim}, Parameters: {num_params:,}\")\n",
    "        \n",
    "        if ndim == 1:\n",
    "            params_1d.append((name, param))\n",
    "        elif ndim == 2:\n",
    "            params_2d.append((name, param))\n",
    "        else:\n",
    "            params_other.append((name, param))\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1D Parameters (e.g., biases, layer norms):\")\n",
    "print(f\"  Count: {len(params_1d)}\")\n",
    "total_1d = sum(p.numel() for _, p in params_1d)\n",
    "print(f\"  Total parameters: {total_1d:,}\")\n",
    "if params_1d:\n",
    "    print(f\"  Names:\")\n",
    "    for name, _ in params_1d:\n",
    "        print(f\"    - {name}\")\n",
    "\n",
    "print(f\"\\n2D Parameters (e.g., weight matrices):\")\n",
    "print(f\"  Count: {len(params_2d)}\")\n",
    "total_2d = sum(p.numel() for _, p in params_2d)\n",
    "print(f\"  Total parameters: {total_2d:,}\")\n",
    "if params_2d:\n",
    "    print(f\"  Names:\")\n",
    "    for name, _ in params_2d:\n",
    "        print(f\"    - {name}\")\n",
    "\n",
    "if params_other:\n",
    "    print(f\"\\nOther Dimensions (3D+):\")\n",
    "    print(f\"  Count: {len(params_other)}\")\n",
    "    total_other = sum(p.numel() for _, p in params_other)\n",
    "    print(f\"  Total parameters: {total_other:,}\")\n",
    "    print(f\"  Names:\")\n",
    "    for name, _ in params_other:\n",
    "        print(f\"    - {name}\")\n",
    "\n",
    "total_params = total_1d + total_2d + (sum(p.numel() for _, p in params_other) if params_other else 0)\n",
    "print(f\"\\nTotal trainable parameters: {total_params:,}\")\n",
    "print(f\"  1D: {total_1d/total_params*100:.2f}%\")\n",
    "print(f\"  2D: {total_2d/total_params*100:.2f}%\")\n",
    "if params_other:\n",
    "    print(f\"  Other: {(total_params - total_1d - total_2d)/total_params*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca1f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All parameters accounted for\n",
      "  Hidden matrix params: 26\n",
      "  Embed params: 3\n",
      "  Scalar params: 50\n",
      "  Total: 78\n"
     ]
    }
   ],
   "source": [
    "# Check if we're counting all parameters correctly\n",
    "hidden_matrix_params = [p for n, p in model.named_parameters() if ((p.ndim >= 2 and \"embed\" not in n) and p.requires_grad )]\n",
    "embed_params = [p for n, p in model.named_parameters() if ((\"embed\" in n or 'cls' in n) and p.requires_grad and p.ndim >= 2)]\n",
    "scalar_params = [p for p in model.parameters() if p.requires_grad and p.ndim < 2]\n",
    "\n",
    "# Verify we're not missing any parameters\n",
    "all_params_counted = set(hidden_matrix_params + embed_params + scalar_params)\n",
    "all_params_actual = set(model.parameters())\n",
    "missing_params = all_params_actual - all_params_counted\n",
    "\n",
    "if missing_params:\n",
    "    print(f\"WARNING: Missing {len(missing_params)} parameters!\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param in missing_params:\n",
    "            print(f\"  Missing: {name} (shape: {param.shape}, ndim: {param.ndim})\")\n",
    "else:\n",
    "    print(\"✓ All parameters accounted for\")\n",
    "    print(f\"  Hidden matrix params: {len(hidden_matrix_params)}\")\n",
    "    print(f\"  Embed params: {len(embed_params)}\")\n",
    "    print(f\"  Scalar params: {len(scalar_params)}\")\n",
    "    print(f\"  Total: {len(all_params_counted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e0da1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ No overlap between embed_params and scalar_params\n"
     ]
    }
   ],
   "source": [
    "# Check for intersection between embed_params and scalar_params\n",
    "embed_set = set(embed_params)\n",
    "scalar_set = set(scalar_params)\n",
    "intersection = embed_set & scalar_set\n",
    "\n",
    "if intersection:\n",
    "    print(f\"WARNING: Found {len(intersection)} parameters in both embed_params and scalar_params!\")\n",
    "    for param in intersection:\n",
    "        for name, p in model.named_parameters():\n",
    "            if p is param:\n",
    "                print(f\"  Overlapping: {name} (shape: {param.shape}, ndim: {param.ndim})\")\n",
    "else:\n",
    "    print(\"✓ No overlap between embed_params and scalar_params\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1b3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_groups = [ dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]\n",
    "# The second line takes each dictionary in adam_groups and creates a new dictionary by:\n",
    "# 1. Unpacking all key-value pairs from the original dict (**g)\n",
    "# 2. Adding/overriding with additional optimizer hyperparameters: betas=(0.8, 0.95), eps=1e-10, use_muon=False\n",
    "# This creates a list of parameter groups with both learning rates and optimizer settings for Adam\n",
    "adam_groups2 = [dict(**g, betas=(0.8, 0.95), eps=1e-10, use_muon=False) for g in adam_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8883b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTORCH_CUDA_ALLOC_CONF\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpandable_segments:True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# prevents a bug on some systems\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor, nn\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch_env/lib/python3.10/site-packages/torch/cuda/__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "with open(sys.argv[0]) as f:\n",
    "    code = f.read() # read the code of this file ASAP, for logging\n",
    "import uuid\n",
    "import time\n",
    "import copy\n",
    "import glob\n",
    "from dataclasses import dataclass\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "torch.empty(1, device=\"cuda\", requires_grad=True).backward() # prevents a bug on some systems\n",
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "# use of FlexAttention contributed by @KoszarskyB\n",
    "from torch.nn.attention.flex_attention import BlockMask, flex_attention\n",
    "#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Custom operators: FP8 matmul by @YouJiacheng\n",
    "\n",
    "@torch.library.custom_op(\"nanogpt::mm\", mutates_args=())\n",
    "def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:\n",
    "    @torch.compile\n",
    "    def impl(x: Tensor, w: Tensor):\n",
    "        assert x.is_contiguous() and w.is_contiguous()\n",
    "        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)\n",
    "        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)\n",
    "        out = torch._scaled_mm(\n",
    "            x_f8,\n",
    "            w_f8.T,\n",
    "            out_dtype=torch.bfloat16,\n",
    "            scale_a=x.new_tensor(x_s, dtype=torch.float32),\n",
    "            scale_b=x.new_tensor(w_s, dtype=torch.float32),\n",
    "            use_fast_accum=True,\n",
    "        )\n",
    "        return out, x_f8, w_f8\n",
    "\n",
    "    return impl(x, w)\n",
    "\n",
    "@mm_op.register_fake\n",
    "def _(x: Tensor, w: Tensor, *_):\n",
    "    assert x.ndim == w.ndim == 2\n",
    "    assert x.shape[1] == w.shape[1]\n",
    "    assert x.device == w.device\n",
    "    assert x.is_contiguous() and w.is_contiguous()\n",
    "    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)\n",
    "\n",
    "@torch.library.custom_op(\"nanogpt::mm_backward\", mutates_args=())\n",
    "def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:\n",
    "    @torch.compile\n",
    "    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):\n",
    "        assert grad.is_contiguous()\n",
    "        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)\n",
    "        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)\n",
    "        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)\n",
    "        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)\n",
    "        grad_x = torch._scaled_mm(\n",
    "            grad_f8,\n",
    "            w_f8.T.contiguous().T,\n",
    "            out_dtype=torch.bfloat16,\n",
    "            scale_a=grad_inv_s,\n",
    "            scale_b=w_inv_s,\n",
    "            use_fast_accum=False,\n",
    "        )\n",
    "        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)\n",
    "        grad_w = torch._scaled_mm(\n",
    "            x_f8.T.contiguous(),\n",
    "            grad_f8.T.contiguous().T,\n",
    "            out_dtype=torch.float32,\n",
    "            scale_a=x_inv_s,\n",
    "            scale_b=grad_inv_s,\n",
    "            use_fast_accum=False,\n",
    "        ).T\n",
    "        return grad_x, grad_w\n",
    "\n",
    "    return impl(g, x_f8, w_f8)\n",
    "\n",
    "@mm_backward_op.register_fake\n",
    "def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):\n",
    "    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)\n",
    "\n",
    "def backward(ctx, grad_out: Tensor, *_):\n",
    "    x_f8, w_f8 = ctx.saved_tensors\n",
    "    x_s, w_s, grad_s = ctx.scales\n",
    "    grad_x, grad_w = torch.ops.nanogpt.mm_backward(\n",
    "        grad_out, x_f8, w_f8, x_s, w_s, grad_s\n",
    "    )\n",
    "    return grad_x, grad_w, None, None, None\n",
    "\n",
    "def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):\n",
    "    *_, x_s, w_s, grad_s = inputs\n",
    "    _, x_f8, w_f8 = output\n",
    "    ctx.save_for_backward(x_f8, w_f8)\n",
    "    ctx.scales = x_s, w_s, grad_s\n",
    "    ctx.set_materialize_grads(False)\n",
    "\n",
    "mm_op.register_autograd(backward, setup_context=setup_context)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PyTorch nn.Module definitions for the model\n",
    "\n",
    "def norm(x: Tensor):\n",
    "    return F.rms_norm(x, (x.size(-1),))\n",
    "\n",
    "class CastedLinear(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):\n",
    "        super().__init__(in_features, out_features, bias=False)\n",
    "        self.use_fp8 = use_fp8\n",
    "        self.x_s = x_s\n",
    "        self.w_s = w_s\n",
    "        self.grad_s = grad_s\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)\n",
    "        bound = (3 ** 0.5) * std\n",
    "        with torch.no_grad():\n",
    "            self.weight.uniform_(-bound, bound)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        if self.use_fp8 and self.training:\n",
    "            _x = x.flatten(0, -2)\n",
    "            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]\n",
    "            return out.reshape(*x.shape[:-1], -1)\n",
    "        else:\n",
    "            return F.linear(x, self.weight.type_as(x))\n",
    "\n",
    "class Rotary(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int):\n",
    "        super().__init__()\n",
    "        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)\n",
    "        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)\n",
    "        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])\n",
    "        t = torch.arange(max_seq_len, dtype=torch.float32)\n",
    "        theta = torch.einsum(\"i,j -> ij\", t, angular_freq)\n",
    "        self.cos = nn.Buffer(theta.cos(), persistent=False)\n",
    "        self.sin = nn.Buffer(theta.sin(), persistent=False)\n",
    "\n",
    "    def forward(self, x_BTHD: Tensor):\n",
    "        assert self.cos.size(0) >= x_BTHD.size(-3)\n",
    "        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]\n",
    "        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)\n",
    "        y1 = x1 * cos + x2 * sin\n",
    "        y2 = x1 * (-sin) + x2 * cos\n",
    "        return torch.cat((y1, y2), 3).type_as(x_BTHD)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        hdim = num_heads * head_dim\n",
    "        std = 0.5 * (dim ** -0.5)\n",
    "        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng\n",
    "        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng\n",
    "        # https://x.com/hi_tysam/status/1879699187107033311\n",
    "        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))\n",
    "        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))\n",
    "        self.rotary = Rotary(head_dim, max_seq_len)\n",
    "        self.c_proj = CastedLinear(hdim, dim)\n",
    "        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977\n",
    "\n",
    "    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):\n",
    "        B, T = x.size(0), x.size(1) # batch size, sequence length\n",
    "        assert B == 1, \"Must use batch size = 1 for FlexAttention\"\n",
    "        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)\n",
    "        q, k = norm(q), norm(k) # QK norm @Grad62304977\n",
    "        q, k = self.rotary(q), self.rotary(k)\n",
    "        if ve is not None:\n",
    "            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977\n",
    "        else: # skip mid-layers token value embeddings by @YouJiacheng\n",
    "            v = self.lambdas[0] * v\n",
    "        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun\n",
    "        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283\n",
    "        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=0.12).transpose(1, 2)\n",
    "        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        hdim = 4 * dim\n",
    "        self.c_fc = CastedLinear(dim, hdim)\n",
    "        self.c_proj = CastedLinear(hdim, dim)\n",
    "        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):\n",
    "        super().__init__()\n",
    "        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng\n",
    "        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None\n",
    "        self.mlp = MLP(dim)\n",
    "        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))\n",
    "\n",
    "    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):\n",
    "        x = self.lambdas[0] * x + self.lambdas[1] * x0\n",
    "        if self.attn is not None:\n",
    "            x = x + self.attn(norm(x), ve, block_mask)\n",
    "        x = x + self.mlp(norm(x))\n",
    "        return x\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# custom buckets for efficient allreduce\n",
    "def initialize_buckets(parameters, bucket_size_bytes):\n",
    "    buckets, current_bucket, current_size = [], [], 0\n",
    "    for param in parameters:\n",
    "        if param.requires_grad and param.grad is None:\n",
    "            param.grad = torch.zeros_like(param)\n",
    "        param_size = param.numel() * param.element_size()\n",
    "        if current_size + param_size > bucket_size_bytes and current_bucket:\n",
    "            buckets.append(current_bucket)\n",
    "            current_bucket = []\n",
    "            current_size = 0\n",
    "        current_bucket.append(param)\n",
    "        current_size += param_size\n",
    "    if current_bucket:\n",
    "        buckets.append(current_bucket)\n",
    "    flat_buffers, bucket_info = [], []\n",
    "    for bucket in buckets:\n",
    "        grad_shapes = [param.grad.shape for param in bucket]\n",
    "        total_elements = sum(param.grad.numel() for param in bucket)\n",
    "        device, dtype = bucket[0].device, bucket[0].dtype\n",
    "        flat_buffer = torch.zeros(total_elements, device=device, dtype=dtype)\n",
    "        flat_buffers.append(flat_buffer)\n",
    "        offsets, offset = [], 0\n",
    "        for param in bucket:\n",
    "            numel = param.grad.numel()\n",
    "            offsets.append((offset, offset + numel))\n",
    "            offset += numel\n",
    "        bucket_info.append({'params': bucket,'shapes': grad_shapes,'offsets': offsets})\n",
    "    return {'bucket_info': bucket_info, 'flat_buffers': flat_buffers}\n",
    "\n",
    "def reduce_gradients(bucket_data):\n",
    "    bucket_info, flat_buffers = bucket_data['bucket_info'], bucket_data['flat_buffers']\n",
    "    handles = []\n",
    "    for i, (info, flat_buffer) in enumerate(zip(bucket_info, flat_buffers)):\n",
    "        for param_idx, param in enumerate(info['params']):\n",
    "            if param.grad is not None:\n",
    "                start, end = info['offsets'][param_idx]\n",
    "                flat_buffer[start:end].copy_(param.grad.view(-1))\n",
    "        handle = dist.all_reduce(flat_buffer, op=dist.ReduceOp.AVG, async_op=True)\n",
    "        handles.append((handle, i))\n",
    "    return handles\n",
    "\n",
    "\n",
    "def unpack_gradients(bucket_data, handles):\n",
    "    bucket_info, flat_buffers = bucket_data['bucket_info'], bucket_data['flat_buffers']\n",
    "    for handle, bucket_idx in handles:\n",
    "        handle.wait()\n",
    "        info, flat_buffer = bucket_info[bucket_idx], flat_buffers[bucket_idx]\n",
    "        for param_idx, param in enumerate(info['params']):\n",
    "            if param.grad is not None:\n",
    "                start, end = info['offsets'][param_idx]\n",
    "                param.grad.copy_(flat_buffer[start:end].view(info['shapes'][param_idx]))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# The main model\n",
    "\n",
    "def next_multiple_of_n(v: float | int, *, n: int):\n",
    "    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, model_dim)\n",
    "        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897\n",
    "        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78\n",
    "        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])\n",
    "        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])\n",
    "        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.\n",
    "        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.\n",
    "        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128),\n",
    "                                    use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)\n",
    "        self.lm_head.weight.detach().zero_() # @Grad62304977\n",
    "        # Add learnable skip connection weights for decoder layers\n",
    "        assert num_layers % 2 == 0\n",
    "        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))\n",
    "\n",
    "    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):\n",
    "        BLOCK_SIZE = 128\n",
    "        docs = (input_seq == 50256).cumsum(0)\n",
    "\n",
    "        def document_causal(b, h, q_idx, kv_idx):\n",
    "            causal_mask = q_idx >= kv_idx\n",
    "            document_mask = docs[q_idx] == docs[kv_idx]\n",
    "            return causal_mask & document_mask\n",
    "\n",
    "        def dense_to_ordered(dense_blockmask: Tensor):\n",
    "            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)\n",
    "            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)\n",
    "            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()\n",
    "\n",
    "        # manual block mask creation by @YouJiacheng\n",
    "        assert len(input_seq) % BLOCK_SIZE == 0\n",
    "        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE\n",
    "        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device=\"cuda\")\n",
    "        causal_blockmask_any = block_idx[:, None] >= block_idx\n",
    "        causal_blockmask_all = block_idx[:, None] > block_idx\n",
    "        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()\n",
    "        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()\n",
    "        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)\n",
    "        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)\n",
    "        blockmask_any = causal_blockmask_any & document_blockmask_any\n",
    "        blockmask_all = causal_blockmask_all & document_blockmask_all\n",
    "        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)\n",
    "        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)\n",
    "        def build_bm(window_size_blocks: Tensor) -> BlockMask:\n",
    "            return BlockMask.from_kv_blocks(\n",
    "                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),\n",
    "                partial_kv_indices,\n",
    "                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),\n",
    "                full_kv_indices,\n",
    "                BLOCK_SIZE=BLOCK_SIZE,\n",
    "                mask_mod=document_causal,\n",
    "            )\n",
    "        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper\n",
    "        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)\n",
    "\n",
    "    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):\n",
    "        assert input_seq.ndim == 1\n",
    "\n",
    "        ve = [value_embed(input_seq) for value_embed in self.value_embeds]\n",
    "        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure\n",
    "        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]\n",
    "        assert len(ve) == len(self.blocks)\n",
    "\n",
    "        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)\n",
    "        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]\n",
    "        assert len(block_masks) == len(self.blocks)\n",
    "\n",
    "        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977\n",
    "\n",
    "        # U-net design by @brendanh0gan\n",
    "        skip_connections = []\n",
    "        n = len(self.skip_weights)\n",
    "        for i in range(len(self.blocks)):\n",
    "            if i >= n:\n",
    "                x = x + self.skip_weights[i - n] * skip_connections.pop()\n",
    "            x = self.blocks[i](x, ve[i], x0, block_masks[i])\n",
    "            if i < n:\n",
    "                skip_connections.append(x)\n",
    "\n",
    "        x = norm(x)\n",
    "        logits = self.lm_head(x).float()\n",
    "        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)\n",
    "        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')\n",
    "        return loss\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Our own simple Distributed Data Loader\n",
    "\n",
    "def _load_data_shard(file: Path):\n",
    "    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32\n",
    "    assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n",
    "    assert header[1] == 1, \"unsupported version\"\n",
    "    num_tokens = int(header[2]) # number of tokens (claimed)\n",
    "    with file.open(\"rb\", buffering=0) as f:\n",
    "        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng\n",
    "        f.seek(256 * 4)\n",
    "        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng\n",
    "        assert nbytes == 2 * num_tokens, \"number of tokens read does not match header\"\n",
    "    return tokens\n",
    "\n",
    "def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):\n",
    "    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]\n",
    "    assert batch_size % world_size == 0\n",
    "    local_batch_size = batch_size // world_size\n",
    "    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training\n",
    "    tokens, pos = _load_data_shard(next(file_iter)), 0\n",
    "    while True:\n",
    "        if pos + batch_size + 1 >= len(tokens):\n",
    "            tokens, pos = _load_data_shard(next(file_iter)), 0\n",
    "        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]\n",
    "        inputs = buf[:-1].to(device=\"cuda\", dtype=torch.int32, non_blocking=True) # no sync on host side;\n",
    "        targets = buf[1:].to(device=\"cuda\", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.\n",
    "        pos += batch_size\n",
    "        yield inputs, targets\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# int main\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    # data\n",
    "    train_files = \"data/fineweb10B/fineweb_train_*.bin\" # input .bin to train on\n",
    "    val_files = \"data/fineweb10B/fineweb_val_*.bin\" # input .bin to eval validation loss on\n",
    "    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons\n",
    "    train_seq_len = 48*1024 # FlexAttention sequence length\n",
    "    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation\n",
    "    # optimization\n",
    "    num_iterations = 1770 # number of iterations to run\n",
    "    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate\n",
    "    # architecture\n",
    "    vocab_size = 50257\n",
    "    # evaluation and logging\n",
    "    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end\n",
    "    save_checkpoint = False\n",
    "args = Hyperparameters()\n",
    "\n",
    "# torchrun sets these env variables\n",
    "rank = int(os.environ[\"RANK\"])\n",
    "world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "assert world_size == 8 # this code is designed for 8xH100\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\", int(os.environ[\"LOCAL_RANK\"]))\n",
    "torch.cuda.set_device(device)\n",
    "dist.init_process_group(backend=\"nccl\", device_id=device)\n",
    "dist.barrier()\n",
    "master_process = (rank == 0) # this process will do logging, checkpointing etc.\n",
    "\n",
    "# begin logging\n",
    "logfile = None\n",
    "if master_process:\n",
    "    run_id = uuid.uuid4()\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    logfile = f\"logs/{run_id}.txt\"\n",
    "    print(logfile)\n",
    "def print0(s, console=False):\n",
    "    if master_process:\n",
    "        with open(logfile, \"a\") as f:\n",
    "            if console:\n",
    "                print(s)\n",
    "            print(s, file=f)\n",
    "\n",
    "# begin by printing this file (the Python code)\n",
    "print0(code)\n",
    "print0(\"=\"*100)\n",
    "# log information about the hardware/software environment this is running on\n",
    "print0(f\"Running Python {sys.version}\")\n",
    "print0(f\"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\")\n",
    "def nvidia_smi():\n",
    "    import subprocess  # avoid top level import\n",
    "    return subprocess.run([\"nvidia-smi\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout\n",
    "print0(nvidia_smi())\n",
    "print0(\"=\"*100)\n",
    "\n",
    "########################################\n",
    "#    Construct model and optimizer     #\n",
    "########################################\n",
    "\n",
    "model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,\n",
    "                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Embedding):\n",
    "        m.bfloat16()\n",
    "for param in model.parameters():\n",
    "    dist.broadcast(param.detach(), 0)\n",
    "\n",
    "# collect the parameters to optimize\n",
    "hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and \"embed\" not in n]\n",
    "embed_params = [p for n, p in model.named_parameters() if \"embed\" in n]\n",
    "scalar_params = [p for p in model.parameters() if p.ndim < 2]\n",
    "head_params = [model.lm_head.weight]\n",
    "\n",
    "# init the optimizer(s)\n",
    "# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence\n",
    "# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094\n",
    "from muon import MuonWithAuxAdam\n",
    "adam_groups = [dict(params=head_params, lr=0.22), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]\n",
    "adam_groups = [dict(**g, betas=(0.8, 0.95), eps=1e-10, use_muon=False) for g in adam_groups]\n",
    "muon_group = dict(params=hidden_matrix_params, lr=0.05, momentum=0.95, use_muon=True)\n",
    "param_groups = [*adam_groups, muon_group]\n",
    "optimizer = MuonWithAuxAdam(param_groups)\n",
    "for group in optimizer.param_groups:\n",
    "    group[\"initial_lr\"] = group[\"lr\"]\n",
    "\n",
    "# init the gradient buckets\n",
    "gradient_buckets = initialize_buckets(model.parameters(), 64.0 * 1024**2) # 128MB buckets\n",
    "\n",
    "# learning rate schedule: stable then decay\n",
    "def get_lr(step: int):\n",
    "    x = step / args.num_iterations # progress in training\n",
    "    assert 0 <= x < 1\n",
    "    if x < 1 - args.cooldown_frac:\n",
    "        return 1.0\n",
    "    else:\n",
    "        w = (1 - x) / args.cooldown_frac\n",
    "        return w * 1.0 + (1 - w) * 0.1\n",
    "\n",
    "# attention window size schedule: linearly increase\n",
    "@lru_cache(1)\n",
    "def get_window_size_blocks_helper(window_size: int):\n",
    "    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)\n",
    "def get_window_size_blocks(step: int):\n",
    "    x = step / args.num_iterations # progress in training\n",
    "    assert 0 <= x <= 1\n",
    "    # Linearly increase the block-wise sliding window size over training 128 -> 1792\n",
    "    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng\n",
    "    window_size = next_multiple_of_n(1728 * x, n=128)\n",
    "    return get_window_size_blocks_helper(window_size)\n",
    "\n",
    "model: nn.Module = torch.compile(model, dynamic=False)\n",
    "\n",
    "########################################\n",
    "#            Warmup kernels            #\n",
    "########################################\n",
    "\n",
    "# Warmup the training kernels, then re-initialize the state so we aren't cheating\n",
    "warmup_steps = 10\n",
    "initial_state = dict(model=copy.deepcopy(model.state_dict()),\n",
    "                     optimizer=copy.deepcopy(optimizer.state_dict())) # save the initial state\n",
    "for _ in range(warmup_steps):\n",
    "    inputs = targets = torch.randint(0, args.vocab_size, size=(args.train_seq_len,), device=\"cuda\")\n",
    "    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()\n",
    "    for param in model.parameters():\n",
    "        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)\n",
    "    optimizer.step()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "model.load_state_dict(initial_state[\"model\"])\n",
    "optimizer.load_state_dict(initial_state[\"optimizer\"])\n",
    "del initial_state\n",
    "\n",
    "########################################\n",
    "#        Training and validation       #\n",
    "########################################\n",
    "\n",
    "train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)\n",
    "training_time_ms = 0\n",
    "# start the clock\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "# begin training\n",
    "train_steps = args.num_iterations\n",
    "for step in range(train_steps + 1):\n",
    "    last_step = (step == train_steps)\n",
    "\n",
    "    # --------------- VALIDATION SECTION -----------------\n",
    "    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):\n",
    "        # stop the clock\n",
    "        torch.cuda.synchronize()\n",
    "        training_time_ms += 1000 * (time.perf_counter() - t0)\n",
    "        model.eval()\n",
    "        val_batch_size = world_size * args.val_seq_len\n",
    "        assert args.val_tokens % val_batch_size == 0\n",
    "        val_steps = args.val_tokens // val_batch_size\n",
    "        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for _ in range(val_steps):\n",
    "                inputs, targets = next(val_loader)\n",
    "                val_loss += model(inputs, targets, get_window_size_blocks(step))\n",
    "        val_loss /= val_steps\n",
    "        del val_loader\n",
    "        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)\n",
    "        print0(f\"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms\", console=True)\n",
    "        model.train()\n",
    "        # start the clock again\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "    if last_step:\n",
    "        if master_process and args.save_checkpoint:\n",
    "            log = dict(step=step, code=code, model=model.state_dict(), optimizer=optimizer.state_dict())\n",
    "            os.makedirs(f\"logs/{run_id}\", exist_ok=True)\n",
    "            torch.save(log, f\"logs/{run_id}/state_step{step:06d}.pt\")\n",
    "        # the last step only has the validation loop, so break to avoid training\n",
    "        break\n",
    "\n",
    "    # --------------- TRAINING SECTION -----------------\n",
    "    inputs, targets = next(train_loader)\n",
    "    model(inputs, targets, get_window_size_blocks(step)).backward()\n",
    "    #for param in model.parameters():\n",
    "    #    dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)\n",
    "    handles = reduce_gradients(gradient_buckets) # does the same thing as commented two lines above, but faster\n",
    "    # set optimization hyperparameters\n",
    "    for group in optimizer.param_groups:\n",
    "        group[\"lr\"] = group[\"initial_lr\"] * get_lr(step)\n",
    "    for group in optimizer.param_groups:\n",
    "        if group[\"use_muon\"]:\n",
    "            frac = min(step / 300, 1) # momentum warmup for muon\n",
    "            group[\"momentum\"] = (1 - frac) * 0.85 + frac * 0.95\n",
    "    # step the optimizer\n",
    "    unpack_gradients(gradient_buckets, handles)\n",
    "    optimizer.step()\n",
    "    # null the gradients\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    # logging\n",
    "    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)\n",
    "    print0(f\"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms\", console=True)\n",
    "\n",
    "print0(f\"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB \"\n",
    "       f\"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB\", console=True)\n",
    "dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b81169",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a9c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
